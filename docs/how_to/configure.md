---
custom_edit_url: https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/configure.ipynb
sidebar_position: 7
keywords: [ConfigurableField, configurable_fields, ConfigurableAlternatives, configurable_alternatives, LCEL]
---
# å¦‚ä½•é…ç½®è¿è¡Œé“¾å†…éƒ¨

:::info Prerequisites

æœ¬æŒ‡å—å‡è®¾æ‚¨ç†Ÿæ‚‰ä»¥ä¸‹æ¦‚å¿µï¼š
- [LangChainè¡¨è¾¾å¼ (LCEL)](/docs/concepts/#langchain-expression-language)
- [é“¾æ¥å¯è¿è¡Œé¡¹](/docs/how_to/sequence/)
- [ç»‘å®šè¿è¡Œæ—¶å‚æ•°](/docs/how_to/binding/)

:::

æœ‰æ—¶æ‚¨å¯èƒ½æƒ³è¦å°è¯•å¤šç§ä¸åŒçš„æ–¹å¼åœ¨æ‚¨çš„é“¾ä¸­è¿›è¡Œæ“ä½œï¼Œç”šè‡³å‘æœ€ç»ˆç”¨æˆ·å±•ç¤ºè¿™äº›æ–¹å¼ã€‚
è¿™å¯ä»¥åŒ…æ‹¬è°ƒæ•´æ¸©åº¦ç­‰å‚æ•°ï¼Œç”šè‡³å°†ä¸€ä¸ªæ¨¡å‹æ›¿æ¢ä¸ºå¦ä¸€ä¸ªæ¨¡å‹ã€‚
ä¸ºäº†ä½¿è¿™ä¸€ä½“éªŒå°½å¯èƒ½ç®€å•ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸¤ç§æ–¹æ³•ã€‚

- ä¸€ä¸ª `configurable_fields` æ–¹æ³•ã€‚å®ƒå…è®¸æ‚¨é…ç½®å¯è¿è¡Œé¡¹çš„ç‰¹å®šå­—æ®µã€‚
- è¿™ä¸å¯è¿è¡Œé¡¹ä¸Šçš„ [`.bind`](/docs/how_to/binding) æ–¹æ³•ç›¸å…³ï¼Œä½†å…è®¸æ‚¨åœ¨è¿è¡Œæ—¶ä¸ºé“¾ä¸­çš„ç‰¹å®šæ­¥éª¤æŒ‡å®šå‚æ•°ï¼Œè€Œä¸æ˜¯äº‹å…ˆæŒ‡å®šã€‚
- ä¸€ä¸ª `configurable_alternatives` æ–¹æ³•ã€‚é€šè¿‡è¿™ä¸ªæ–¹æ³•ï¼Œæ‚¨å¯ä»¥åˆ—å‡ºä»»ä½•ç‰¹å®šå¯è¿è¡Œé¡¹çš„æ›¿ä»£é€‰é¡¹ï¼Œè¿™äº›é€‰é¡¹å¯ä»¥åœ¨è¿è¡Œæ—¶è®¾ç½®ï¼Œå¹¶å°†å…¶æ›¿æ¢ä¸ºæŒ‡å®šçš„æ›¿ä»£é€‰é¡¹ã€‚

## å¯é…ç½®å­—æ®µ

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç¤ºä¾‹æ¥æ¼”ç¤ºå¦‚ä½•åœ¨è¿è¡Œæ—¶é…ç½®èŠå¤©æ¨¡å‹å­—æ®µï¼Œä¾‹å¦‚æ¸©åº¦ï¼š


```python
%pip install --upgrade --quiet langchain langchain-openai

import os
from getpass import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()
```


```python
<!--IMPORTS:[{"imported": "PromptTemplate", "source": "langchain_core.prompts", "docs": "https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html", "title": "How to configure runtime chain internals"}, {"imported": "ConfigurableField", "source": "langchain_core.runnables", "docs": "https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.utils.ConfigurableField.html", "title": "How to configure runtime chain internals"}, {"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "How to configure runtime chain internals"}]-->
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import ConfigurableField
from langchain_openai import ChatOpenAI

model = ChatOpenAI(temperature=0).configurable_fields(
    temperature=ConfigurableField(
        id="llm_temperature",
        name="LLM Temperature",
        description="The temperature of the LLM",
    )
)

model.invoke("pick a random number")
```



```output
AIMessage(content='17', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 11, 'total_tokens': 12}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-ba26a0da-0a69-4533-ab7f-21178a73d303-0')
```


ä¸Šé¢ï¼Œæˆ‘ä»¬å°† `temperature` å®šä¹‰ä¸ºä¸€ä¸ª [`ConfigurableField`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.utils.ConfigurableField.html#langchain_core.runnables.utils.ConfigurableField)ï¼Œå¯ä»¥åœ¨è¿è¡Œæ—¶è®¾ç½®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ [`with_config`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_config) æ–¹æ³•ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š


```python
model.with_config(configurable={"llm_temperature": 0.9}).invoke("pick a random number")
```



```output
AIMessage(content='12', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 11, 'total_tokens': 12}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-ba8422ad-be77-4cb1-ac45-ad0aae74e3d9-0')
```


è¯·æ³¨æ„ï¼Œå­—å…¸ä¸­ä¼ é€’çš„ `llm_temperature` æ¡ç›®ä¸ `ConfigurableField` çš„ `id` å…·æœ‰ç›¸åŒçš„é”®ã€‚

æˆ‘ä»¬ä¹Ÿå¯ä»¥è¿™æ ·åšï¼Œä»¥å½±å“é“¾ä¸­ä»…ä¸€ä¸ªæ­¥éª¤ï¼š


```python
prompt = PromptTemplate.from_template("Pick a random number above {x}")
chain = prompt | model

chain.invoke({"x": 0})
```



```output
AIMessage(content='27', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 14, 'total_tokens': 15}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-ecd4cadd-1b72-4f92-b9a0-15e08091f537-0')
```



```python
chain.with_config(configurable={"llm_temperature": 0.9}).invoke({"x": 0})
```



```output
AIMessage(content='35', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 14, 'total_tokens': 15}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-a916602b-3460-46d3-a4a8-7c926ec747c0-0')
```


### ä½¿ç”¨ HubRunnables

è¿™å¯¹äºå…è®¸åˆ‡æ¢æç¤ºè¯éå¸¸æœ‰ç”¨


```python
<!--IMPORTS:[{"imported": "HubRunnable", "source": "langchain.runnables.hub", "docs": "https://python.langchain.com/api_reference/langchain/runnables/langchain.runnables.hub.HubRunnable.html", "title": "How to configure runtime chain internals"}]-->
from langchain.runnables.hub import HubRunnable

prompt = HubRunnable("rlm/rag-prompt").configurable_fields(
    owner_repo_commit=ConfigurableField(
        id="hub_commit",
        name="Hub Commit",
        description="The Hub commit to pull from",
    )
)

prompt.invoke({"question": "foo", "context": "bar"})
```



```output
ChatPromptValue(messages=[HumanMessage(content="You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: foo \nContext: bar \nAnswer:")])
```



```python
prompt.with_config(configurable={"hub_commit": "rlm/rag-prompt-llama"}).invoke(
    {"question": "foo", "context": "bar"}
)
```



```output
ChatPromptValue(messages=[HumanMessage(content="[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>> \nQuestion: foo \nContext: bar \nAnswer: [/INST]")])
```


## å¯é…ç½®çš„æ›¿ä»£æ–¹æ¡ˆ



`configurable_alternatives()` æ–¹æ³•å…è®¸æˆ‘ä»¬ç”¨æ›¿ä»£æ–¹æ¡ˆæ›¿æ¢é“¾ä¸­çš„æ­¥éª¤ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬å°†ä¸€ä¸ªèŠå¤©æ¨¡å‹æ›¿æ¢ä¸ºå¦ä¸€ä¸ªï¼š


```python
%pip install --upgrade --quiet langchain-anthropic

import os
from getpass import getpass

if "ANTHROPIC_API_KEY" not in os.environ:
    os.environ["ANTHROPIC_API_KEY"] = getpass()
```
```output
[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.
You should consider upgrading via the '/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip' command.[0m[33m
[0mNote: you may need to restart the kernel to use updated packages.
```

```python
<!--IMPORTS:[{"imported": "ChatAnthropic", "source": "langchain_anthropic", "docs": "https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html", "title": "How to configure runtime chain internals"}, {"imported": "PromptTemplate", "source": "langchain_core.prompts", "docs": "https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html", "title": "How to configure runtime chain internals"}, {"imported": "ConfigurableField", "source": "langchain_core.runnables", "docs": "https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.utils.ConfigurableField.html", "title": "How to configure runtime chain internals"}, {"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "How to configure runtime chain internals"}]-->
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import ConfigurableField
from langchain_openai import ChatOpenAI

llm = ChatAnthropic(
    model="claude-3-haiku-20240307", temperature=0
).configurable_alternatives(
    # This gives this field an id
    # When configuring the end runnable, we can then use this id to configure this field
    ConfigurableField(id="llm"),
    # This sets a default_key.
    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used
    default_key="anthropic",
    # This adds a new option, with name `openai` that is equal to `ChatOpenAI()`
    openai=ChatOpenAI(),
    # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model="gpt-4")`
    gpt4=ChatOpenAI(model="gpt-4"),
    # You can add more configuration options here
)
prompt = PromptTemplate.from_template("Tell me a joke about {topic}")
chain = prompt | llm

# By default it will call Anthropic
chain.invoke({"topic": "bears"})
```



```output
AIMessage(content="Here's a bear joke for you:\n\nWhy don't bears wear socks? \nBecause they have bear feet!\n\nHow's that? I tried to come up with a simple, silly pun-based joke about bears. Puns and wordplay are a common way to create humorous bear jokes. Let me know if you'd like to hear another one!", response_metadata={'id': 'msg_018edUHh5fUbWdiimhrC3dZD', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 80}}, id='run-775bc58c-28d7-4e6b-a268-48fa6661f02f-0')
```



```python
# We can use `.with_config(configurable={"llm": "openai"})` to specify an llm to use
chain.with_config(configurable={"llm": "openai"}).invoke({"topic": "bears"})
```



```output
AIMessage(content="Why don't bears like fast food?\n\nBecause they can't catch it!", response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 13, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-7bdaa992-19c9-4f0d-9a0c-1f326bc992d4-0')
```



```python
# If we use the `default_key` then it uses the default
chain.with_config(configurable={"llm": "anthropic"}).invoke({"topic": "bears"})
```



```output
AIMessage(content="Here's a bear joke for you:\n\nWhy don't bears wear socks? \nBecause they have bear feet!\n\nHow's that? I tried to come up with a simple, silly pun-based joke about bears. Puns and wordplay are a common way to create humorous bear jokes. Let me know if you'd like to hear another one!", response_metadata={'id': 'msg_01BZvbmnEPGBtcxRWETCHkct', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 80}}, id='run-59b6ee44-a1cd-41b8-a026-28ee67cdd718-0')
```


### ä½¿ç”¨æç¤ºè¯

æˆ‘ä»¬å¯ä»¥åšç±»ä¼¼çš„äº‹æƒ…ï¼Œä½†åœ¨æç¤ºè¯ä¹‹é—´äº¤æ›¿



```python
llm = ChatAnthropic(model="claude-3-haiku-20240307", temperature=0)
prompt = PromptTemplate.from_template(
    "Tell me a joke about {topic}"
).configurable_alternatives(
    # This gives this field an id
    # When configuring the end runnable, we can then use this id to configure this field
    ConfigurableField(id="prompt"),
    # This sets a default_key.
    # If we specify this key, the default prompt (asking for a joke, as initialized above) will be used
    default_key="joke",
    # This adds a new option, with name `poem`
    poem=PromptTemplate.from_template("Write a short poem about {topic}"),
    # You can add more configuration options here
)
chain = prompt | llm

# By default it will write a joke
chain.invoke({"topic": "bears"})
```



```output
AIMessage(content="Here's a bear joke for you:\n\nWhy don't bears wear socks? \nBecause they have bear feet!", response_metadata={'id': 'msg_01DtM1cssjNFZYgeS3gMZ49H', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 28}}, id='run-8199af7d-ea31-443d-b064-483693f2e0a1-0')
```



```python
# We can configure it write a poem
chain.with_config(configurable={"prompt": "poem"}).invoke({"topic": "bears"})
```



```output
AIMessage(content="Here is a short poem about bears:\n\nMajestic bears, strong and true,\nRoaming the forests, wild and free.\nPowerful paws, fur soft and brown,\nCommanding respect, nature's crown.\n\nForaging for berries, fishing streams,\nProtecting their young, fierce and keen.\nMighty bears, a sight to behold,\nGuardians of the wilderness, untold.\n\nIn the wild they reign supreme,\nEmbodying nature's grand theme.\nBears, a symbol of strength and grace,\nCaptivating all who see their face.", response_metadata={'id': 'msg_01Wck3qPxrjURtutvtodaJFn', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 134}}, id='run-69414a1e-51d7-4bec-a307-b34b7d61025e-0')
```


### ä½¿ç”¨æç¤ºè¯å’Œå¤§å‹è¯­è¨€æ¨¡å‹

æˆ‘ä»¬è¿˜å¯ä»¥é…ç½®å¤šä¸ªé€‰é¡¹ï¼
è¿™æ˜¯ä¸€ä¸ªåŒæ—¶ä½¿ç”¨æç¤ºè¯å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„ç¤ºä¾‹ã€‚


```python
llm = ChatAnthropic(
    model="claude-3-haiku-20240307", temperature=0
).configurable_alternatives(
    # This gives this field an id
    # When configuring the end runnable, we can then use this id to configure this field
    ConfigurableField(id="llm"),
    # This sets a default_key.
    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used
    default_key="anthropic",
    # This adds a new option, with name `openai` that is equal to `ChatOpenAI()`
    openai=ChatOpenAI(),
    # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model="gpt-4")`
    gpt4=ChatOpenAI(model="gpt-4"),
    # You can add more configuration options here
)
prompt = PromptTemplate.from_template(
    "Tell me a joke about {topic}"
).configurable_alternatives(
    # This gives this field an id
    # When configuring the end runnable, we can then use this id to configure this field
    ConfigurableField(id="prompt"),
    # This sets a default_key.
    # If we specify this key, the default prompt (asking for a joke, as initialized above) will be used
    default_key="joke",
    # This adds a new option, with name `poem`
    poem=PromptTemplate.from_template("Write a short poem about {topic}"),
    # You can add more configuration options here
)
chain = prompt | llm

# We can configure it write a poem with OpenAI
chain.with_config(configurable={"prompt": "poem", "llm": "openai"}).invoke(
    {"topic": "bears"}
)
```



```output
AIMessage(content="In the forest deep and wide,\nBears roam with grace and pride.\nWith fur as dark as night,\nThey rule the land with all their might.\n\nIn winter's chill, they hibernate,\nIn spring they emerge, hungry and great.\nWith claws sharp and eyes so keen,\nThey hunt for food, fierce and lean.\n\nBut beneath their tough exterior,\nLies a gentle heart, warm and superior.\nThey love their cubs with all their might,\nProtecting them through day and night.\n\nSo let us admire these majestic creatures,\nIn awe of their strength and features.\nFor in the wild, they reign supreme,\nThe mighty bears, a timeless dream.", response_metadata={'token_usage': {'completion_tokens': 133, 'prompt_tokens': 13, 'total_tokens': 146}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-5eec0b96-d580-49fd-ac4e-e32a0803b49b-0')
```



```python
# We can always just configure only one if we want
chain.with_config(configurable={"llm": "openai"}).invoke({"topic": "bears"})
```



```output
AIMessage(content="Why don't bears wear shoes?\n\nBecause they have bear feet!", response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 13, 'total_tokens': 26}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-c1b14c9c-4988-49b8-9363-15bfd479973a-0')
```


### ä¿å­˜é…ç½®

æˆ‘ä»¬è¿˜å¯ä»¥è½»æ¾åœ°å°†é…ç½®å¥½çš„é“¾ä¿å­˜ä¸ºå®ƒä»¬è‡ªå·±çš„å¯¹è±¡


```python
openai_joke = chain.with_config(configurable={"llm": "openai"})

openai_joke.invoke({"topic": "bears"})
```



```output
AIMessage(content="Why did the bear break up with his girlfriend? \nBecause he couldn't bear the relationship anymore!", response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 13, 'total_tokens': 33}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-391ebd55-9137-458b-9a11-97acaff6a892-0')
```


## ä¸‹ä¸€æ­¥

æ‚¨ç°åœ¨çŸ¥é“å¦‚ä½•åœ¨è¿è¡Œæ—¶é…ç½®é“¾çš„å†…éƒ¨æ­¥éª¤ã€‚

è¦äº†è§£æ›´å¤šï¼Œè¯·å‚é˜…æœ¬èŠ‚ä¸­å…³äºå¯è¿è¡Œé¡¹çš„å…¶ä»–ä½¿ç”¨æŒ‡å—ï¼ŒåŒ…æ‹¬ï¼š

- ä½¿ç”¨ [.bind()](/docs/how_to/binding) ä½œä¸ºè®¾ç½®å¯è¿è¡Œé¡¹è¿è¡Œæ—¶å‚æ•°çš„æ›´ç®€å•æ–¹æ³•
