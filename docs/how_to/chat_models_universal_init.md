---
custom_edit_url: https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/chat_models_universal_init.ipynb
---
# å¦‚ä½•åœ¨ä¸€è¡Œä¸­åˆå§‹åŒ–ä»»ä½•æ¨¡å‹

è®¸å¤šå¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨ç¨‹åºå…è®¸æœ€ç»ˆç”¨æˆ·æŒ‡å®šä»–ä»¬å¸Œæœ›åº”ç”¨ç¨‹åºä½¿ç”¨çš„å¤§æ¨¡å‹ä¾›åº”å•†å’Œæ¨¡å‹ã€‚è¿™éœ€è¦ç¼–å†™ä¸€äº›é€»è¾‘ï¼Œæ ¹æ®ç”¨æˆ·é…ç½®åˆå§‹åŒ–ä¸åŒçš„èŠå¤©æ¨¡å‹ã€‚`init_chat_model()` è¾…åŠ©æ–¹æ³•ä½¿å¾—åˆå§‹åŒ–å¤šç§ä¸åŒæ¨¡å‹é›†æˆå˜å¾—ç®€å•ï¼Œè€Œæ— éœ€æ‹…å¿ƒå¯¼å…¥è·¯å¾„å’Œç±»åã€‚

:::tip Supported models

è¯·å‚é˜… [init_chat_model()](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html) API å‚è€ƒï¼Œä»¥è·å–æ”¯æŒçš„é›†æˆçš„å®Œæ•´åˆ—è¡¨ã€‚

ç¡®ä¿æ‚¨å·²å®‰è£…ä»»ä½•å¸Œæœ›æ”¯æŒçš„å¤§æ¨¡å‹ä¾›åº”å•†çš„é›†æˆåŒ…ã€‚ä¾‹å¦‚ï¼Œæ‚¨åº”è¯¥å®‰è£… `langchain-openai` ä»¥åˆå§‹åŒ– OpenAI æ¨¡å‹ã€‚

:::


```python
%pip install -qU langchain>=0.2.8 langchain-openai langchain-anthropic langchain-google-vertexai
```

## åŸºæœ¬ç”¨æ³•


```python
<!--IMPORTS:[{"imported": "init_chat_model", "source": "langchain.chat_models", "docs": "https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html", "title": "How to init any model in one line"}]-->
from langchain.chat_models import init_chat_model

# Returns a langchain_openai.ChatOpenAI instance.
gpt_4o = init_chat_model("gpt-4o", model_provider="openai", temperature=0)
# Returns a langchain_anthropic.ChatAnthropic instance.
claude_opus = init_chat_model(
    "claude-3-opus-20240229", model_provider="anthropic", temperature=0
)
# Returns a langchain_google_vertexai.ChatVertexAI instance.
gemini_15 = init_chat_model(
    "gemini-1.5-pro", model_provider="google_vertexai", temperature=0
)

# Since all model integrations implement the ChatModel interface, you can use them in the same way.
print("GPT-4o: " + gpt_4o.invoke("what's your name").content + "\n")
print("Claude Opus: " + claude_opus.invoke("what's your name").content + "\n")
print("Gemini 1.5: " + gemini_15.invoke("what's your name").content + "\n")
```
```output
/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_95293/571506279.py:4: LangChainBetaWarning: The function `init_chat_model` is in beta. It is actively being worked on, so the API may change.
  gpt_4o = init_chat_model("gpt-4o", model_provider="openai", temperature=0)
``````output
GPT-4o: I'm an AI created by OpenAI, and I don't have a personal name. How can I assist you today?
``````output
Claude Opus: My name is Claude. It's nice to meet you!
``````output
Gemini 1.5: I am a large language model, trained by Google. 

I don't have a name like a person does. You can call me Bard if you like! ğŸ˜Š
```
## æ¨æ–­å¤§æ¨¡å‹ä¾›åº”å•†

å¯¹äºå¸¸è§å’Œä¸åŒçš„æ¨¡å‹åç§°ï¼Œ`init_chat_model()` å°†å°è¯•æ¨æ–­å¤§æ¨¡å‹ä¾›åº”å•†ã€‚æœ‰å…³æ¨æ–­è¡Œä¸ºçš„å®Œæ•´åˆ—è¡¨ï¼Œè¯·å‚è§ [API å‚è€ƒ](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html)ã€‚ä¾‹å¦‚ï¼Œä»»ä½•ä»¥ `gpt-3...` æˆ– `gpt-4...` å¼€å¤´çš„æ¨¡å‹å°†è¢«æ¨æ–­ä¸ºä½¿ç”¨å¤§æ¨¡å‹ä¾›åº”å•† `openai`ã€‚


```python
gpt_4o = init_chat_model("gpt-4o", temperature=0)
claude_opus = init_chat_model("claude-3-opus-20240229", temperature=0)
gemini_15 = init_chat_model("gemini-1.5-pro", temperature=0)
```

## åˆ›å»ºå¯é…ç½®æ¨¡å‹

æ‚¨è¿˜å¯ä»¥é€šè¿‡æŒ‡å®š `configurable_fields` æ¥åˆ›å»ºä¸€ä¸ªè¿è¡Œæ—¶å¯é…ç½®çš„æ¨¡å‹ã€‚å¦‚æœæ‚¨ä¸æŒ‡å®š `model` å€¼ï¼Œåˆ™â€œmodelâ€å’Œâ€œmodel_providerâ€å°†é»˜è®¤å¯é…ç½®ã€‚


```python
configurable_model = init_chat_model(temperature=0)

configurable_model.invoke(
    "what's your name", config={"configurable": {"model": "gpt-4o"}}
)
```



```output
AIMessage(content="I'm an AI created by OpenAI, and I don't have a personal name. How can I assist you today?", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 11, 'total_tokens': 34}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-b41df187-4627-490d-af3c-1c96282d3eb0-0', usage_metadata={'input_tokens': 11, 'output_tokens': 23, 'total_tokens': 34})
```



```python
configurable_model.invoke(
    "what's your name", config={"configurable": {"model": "claude-3-5-sonnet-20240620"}}
)
```



```output
AIMessage(content="My name is Claude. It's nice to meet you!", additional_kwargs={}, response_metadata={'id': 'msg_01Fx9P74A7syoFkwE73CdMMY', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 11, 'output_tokens': 15}}, id='run-a0fd2bbd-3b7e-46bf-8d69-a48c7e60b03c-0', usage_metadata={'input_tokens': 11, 'output_tokens': 15, 'total_tokens': 26})
```


### å¸¦æœ‰é»˜è®¤å€¼çš„å¯é…ç½®æ¨¡å‹

æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªå¸¦æœ‰é»˜è®¤æ¨¡å‹å€¼çš„å¯é…ç½®æ¨¡å‹ï¼ŒæŒ‡å®šå“ªäº›å‚æ•°æ˜¯å¯é…ç½®çš„ï¼Œå¹¶ä¸ºå¯é…ç½®å‚æ•°æ·»åŠ å‰ç¼€ï¼š


```python
first_llm = init_chat_model(
    model="gpt-4o",
    temperature=0,
    configurable_fields=("model", "model_provider", "temperature", "max_tokens"),
    config_prefix="first",  # useful when you have a chain with multiple models
)

first_llm.invoke("what's your name")
```



```output
AIMessage(content="I'm an AI created by OpenAI, and I don't have a personal name. How can I assist you today?", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 11, 'total_tokens': 34}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-3380f977-4b89-4f44-bc02-b64043b3166f-0', usage_metadata={'input_tokens': 11, 'output_tokens': 23, 'total_tokens': 34})
```



```python
first_llm.invoke(
    "what's your name",
    config={
        "configurable": {
            "first_model": "claude-3-5-sonnet-20240620",
            "first_temperature": 0.5,
            "first_max_tokens": 100,
        }
    },
)
```



```output
AIMessage(content="My name is Claude. It's nice to meet you!", additional_kwargs={}, response_metadata={'id': 'msg_01EFKSWpmsn2PSYPQa4cNHWb', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 11, 'output_tokens': 15}}, id='run-3c58f47c-41b9-4e56-92e7-fb9602e3787c-0', usage_metadata={'input_tokens': 11, 'output_tokens': 15, 'total_tokens': 26})
```


### ä»¥å£°æ˜æ–¹å¼ä½¿ç”¨å¯é…ç½®æ¨¡å‹

æˆ‘ä»¬å¯ä»¥åœ¨å¯é…ç½®æ¨¡å‹ä¸Šè°ƒç”¨å£°æ˜æ€§æ“ä½œï¼Œå¦‚ `bind_tools`ã€`with_structured_output`ã€`with_configurable` ç­‰ï¼Œå¹¶ä»¥ä¸å¸¸è§„å®ä¾‹åŒ–çš„èŠå¤©æ¨¡å‹å¯¹è±¡ç›¸åŒçš„æ–¹å¼é“¾æ¥å¯é…ç½®æ¨¡å‹ã€‚


```python
from pydantic import BaseModel, Field


class GetWeather(BaseModel):
    """Get the current weather in a given location"""

    location: str = Field(..., description="The city and state, e.g. San Francisco, CA")


class GetPopulation(BaseModel):
    """Get the current population in a given location"""

    location: str = Field(..., description="The city and state, e.g. San Francisco, CA")


llm = init_chat_model(temperature=0)
llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])

llm_with_tools.invoke(
    "what's bigger in 2024 LA or NYC", config={"configurable": {"model": "gpt-4o"}}
).tool_calls
```



```output
[{'name': 'GetPopulation',
  'args': {'location': 'Los Angeles, CA'},
  'id': 'call_Ga9m8FAArIyEjItHmztPYA22',
  'type': 'tool_call'},
 {'name': 'GetPopulation',
  'args': {'location': 'New York, NY'},
  'id': 'call_jh2dEvBaAHRaw5JUDthOs7rt',
  'type': 'tool_call'}]
```



```python
llm_with_tools.invoke(
    "what's bigger in 2024 LA or NYC",
    config={"configurable": {"model": "claude-3-5-sonnet-20240620"}},
).tool_calls
```



```output
[{'name': 'GetPopulation',
  'args': {'location': 'Los Angeles, CA'},
  'id': 'toolu_01JMufPf4F4t2zLj7miFeqXp',
  'type': 'tool_call'},
 {'name': 'GetPopulation',
  'args': {'location': 'New York City, NY'},
  'id': 'toolu_01RQBHcE8kEEbYTuuS8WqY1u',
  'type': 'tool_call'}]
```

