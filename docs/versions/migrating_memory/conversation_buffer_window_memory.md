---
custom_edit_url: https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/migrating_memory/conversation_buffer_window_memory.ipynb
---
# ä» ConversationBufferWindowMemory æˆ– ConversationTokenBufferMemory è¿ç§»

å¦‚æœæ‚¨å°è¯•ä»ä¸‹é¢åˆ—å‡ºçš„æ—§å†…å­˜ç±»è¿ç§»ï¼Œè¯·éµå¾ªæœ¬æŒ‡å—ï¼š


| å†…å­˜ç±»å‹                          | æè¿°                                                                                                                                                       |
|----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `ConversationBufferWindowMemory` | ä¿ç•™å¯¹è¯çš„æœ€å `n` æ¡æ¶ˆæ¯ã€‚å½“æ¶ˆæ¯æ•°é‡è¶…è¿‡ `n` æ¡æ—¶ï¼Œåˆ é™¤æœ€æ—§çš„æ¶ˆæ¯ã€‚                                                                      |
| `ConversationTokenBufferMemory`  | ä»…ä¿ç•™å¯¹è¯ä¸­æœ€è¿‘çš„æ¶ˆæ¯ï¼Œå‰ææ˜¯å¯¹è¯ä¸­çš„æ€»ä»¤ç‰Œæ•°ä¸è¶…è¿‡æŸä¸ªé™åˆ¶ã€‚ |

`ConversationBufferWindowMemory` å’Œ `ConversationTokenBufferMemory` åœ¨åŸå§‹å¯¹è¯å†å²ä¸Šåº”ç”¨é¢å¤–å¤„ç†ï¼Œä»¥å°†å¯¹è¯å†å²ä¿®å‰ªåˆ°é€‚åˆèŠå¤©æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£çš„å¤§å°ã€‚

æ­¤å¤„ç†åŠŸèƒ½å¯ä»¥ä½¿ç”¨ LangChain å†…ç½®çš„ [trim_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) å‡½æ•°æ¥å®ç°ã€‚

:::important

æˆ‘ä»¬å°†é¦–å…ˆæ¢ç´¢ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œæ¶‰åŠå¯¹æ•´ä¸ªå¯¹è¯å†å²åº”ç”¨å¤„ç†é€»è¾‘ã€‚

è™½ç„¶è¿™ç§æ–¹æ³•æ˜“äºå®ç°ï¼Œä½†å®ƒæœ‰ä¸€ä¸ªç¼ºç‚¹ï¼šéšç€å¯¹è¯çš„å¢é•¿ï¼Œå»¶è¿Ÿä¹Ÿä¼šå¢åŠ ï¼Œå› ä¸ºé€»è¾‘åœ¨æ¯æ¬¡è½®æ¢æ—¶éƒ½ä¼šé‡æ–°åº”ç”¨äºå¯¹è¯ä¸­çš„æ‰€æœ‰å…ˆå‰äº¤æµã€‚

æ›´é«˜çº§çš„ç­–ç•¥ä¸“æ³¨äºé€æ­¥æ›´æ–°å¯¹è¯å†å²ï¼Œä»¥é¿å…å†—ä½™å¤„ç†ã€‚

ä¾‹å¦‚ï¼Œlanggraph [å…³äºæ‘˜è¦çš„æ“ä½œæŒ‡å—](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/) æ¼”ç¤ºäº†
å¦‚ä½•ç»´æŠ¤å¯¹è¯çš„å®æ—¶æ‘˜è¦ï¼ŒåŒæ—¶ä¸¢å¼ƒè¾ƒæ—§çš„æ¶ˆæ¯ï¼Œç¡®ä¿å®ƒä»¬åœ¨åç»­å›åˆä¸­ä¸ä¼šè¢«é‡æ–°å¤„ç†ã€‚
:::

## è®¾ç½®


```python
%%capture --no-stderr
%pip install --upgrade --quiet langchain-openai langchain
```


```python
import os
from getpass import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()
```

## ä½¿ç”¨ LLMChain / ä¼šè¯é“¾çš„é—ç•™ç”¨æ³•

<details open>


```python
<!--IMPORTS:[{"imported": "LLMChain", "source": "langchain.chains", "docs": "https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "ConversationBufferWindowMemory", "source": "langchain.memory", "docs": "https://python.langchain.com/api_reference/langchain/memory/langchain.memory.buffer_window.ConversationBufferWindowMemory.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "SystemMessage", "source": "langchain_core.messages", "docs": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "ChatPromptTemplate", "source": "langchain_core.prompts", "docs": "https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "ChatPromptTemplate", "source": "langchain_core.prompts.chat", "docs": "https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "HumanMessagePromptTemplate", "source": "langchain_core.prompts.chat", "docs": "https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.HumanMessagePromptTemplate.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "MessagesPlaceholder", "source": "langchain_core.prompts.chat", "docs": "https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}]-->
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferWindowMemory
from langchain_core.messages import SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts.chat import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain_openai import ChatOpenAI

prompt = ChatPromptTemplate(
    [
        SystemMessage(content="You are a helpful assistant."),
        MessagesPlaceholder(variable_name="chat_history"),
        HumanMessagePromptTemplate.from_template("{text}"),
    ]
)

# highlight-start
memory = ConversationBufferWindowMemory(memory_key="chat_history", return_messages=True)
# highlight-end

legacy_chain = LLMChain(
    llm=ChatOpenAI(),
    prompt=prompt,
    # highlight-next-line
    memory=memory,
)

legacy_result = legacy_chain.invoke({"text": "my name is bob"})
print(legacy_result)

legacy_result = legacy_chain.invoke({"text": "what was my name"})
print(legacy_result)
```
```output
{'text': 'Nice to meet you, Bob! How can I assist you today?', 'chat_history': []}
{'text': 'Your name is Bob. How can I assist you further, Bob?', 'chat_history': [HumanMessage(content='my name is bob', additional_kwargs={}, response_metadata={}), AIMessage(content='Nice to meet you, Bob! How can I assist you today?', additional_kwargs={}, response_metadata={})]}
```
</details>

## é‡æ–°å®ç° ConversationBufferWindowMemory é€»è¾‘

è®©æˆ‘ä»¬é¦–å…ˆåˆ›å»ºé€‚å½“çš„é€»è¾‘æ¥å¤„ç†èŠå¤©å†å²ï¼Œç„¶åæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å°†å…¶é›†æˆåˆ°åº”ç”¨ç¨‹åºä¸­ã€‚æ‚¨å¯ä»¥ç¨åç”¨æ›´é«˜çº§çš„é€»è¾‘æ›¿æ¢æ­¤åŸºæœ¬è®¾ç½®ï¼Œä»¥æ»¡è¶³æ‚¨çš„ç‰¹å®šéœ€æ±‚ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ `trim_messages` æ¥å®ç°é€»è¾‘ï¼Œä»¥ä¿ç•™èŠå¤©çš„æœ€å `n` æ¡æ¶ˆæ¯ã€‚å½“æ¶ˆæ¯æ•°é‡è¶…è¿‡ `n` æ—¶ï¼Œå®ƒå°†åˆ é™¤æœ€æ—§çš„æ¶ˆæ¯ã€‚

æ­¤å¤–ï¼Œå¦‚æœç³»ç»Ÿæ¶ˆæ¯å­˜åœ¨ï¼Œæˆ‘ä»¬ä¹Ÿå°†ä¿ç•™å®ƒ -- å½“å­˜åœ¨æ—¶ï¼Œå®ƒæ˜¯åŒ…å«èŠå¤©æ¨¡å‹æŒ‡ä»¤çš„ä¼šè¯ä¸­çš„ç¬¬ä¸€æ¡æ¶ˆæ¯ã€‚


```python
<!--IMPORTS:[{"imported": "AIMessage", "source": "langchain_core.messages", "docs": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "BaseMessage", "source": "langchain_core.messages", "docs": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "SystemMessage", "source": "langchain_core.messages", "docs": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "trim_messages", "source": "langchain_core.messages", "docs": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}]-->
from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
    trim_messages,
)
from langchain_openai import ChatOpenAI

messages = [
    SystemMessage("you're a good assistant, you always respond with a joke."),
    HumanMessage("i wonder why it's called langchain"),
    AIMessage(
        'Well, I guess they thought "WordRope" and "SentenceString" just didn\'t have the same ring to it!'
    ),
    HumanMessage("and who is harrison chasing anyways"),
    AIMessage(
        "Hmmm let me think.\n\nWhy, he's probably chasing after the last cup of coffee in the office!"
    ),
    HumanMessage("why is 42 always the answer?"),
    AIMessage(
        "Because itâ€™s the only number thatâ€™s constantly right, even when it doesnâ€™t add up!"
    ),
    HumanMessage("What did the cow say?"),
]
```


```python
<!--IMPORTS:[{"imported": "trim_messages", "source": "langchain_core.messages", "docs": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}]-->
from langchain_core.messages import trim_messages

selected_messages = trim_messages(
    messages,
    token_counter=len,  # <-- len will simply count the number of messages rather than tokens
    max_tokens=5,  # <-- allow up to 5 messages.
    strategy="last",
    # The start_on is specified
    # to make sure we do not generate a sequence where
    # a ToolMessage that contains the result of a tool invocation
    # appears before the AIMessage that requested a tool invocation
    # as this will cause some chat models to raise an error.
    start_on=("human", "ai"),
    include_system=True,  # <-- Keep the system message
    allow_partial=False,
)

for msg in selected_messages:
    msg.pretty_print()
```
```output
================================[1m System Message [0m================================

you're a good assistant, you always respond with a joke.
==================================[1m Ai Message [0m==================================

Hmmm let me think.

Why, he's probably chasing after the last cup of coffee in the office!
================================[1m Human Message [0m=================================

why is 42 always the answer?
==================================[1m Ai Message [0m==================================

Because itâ€™s the only number thatâ€™s constantly right, even when it doesnâ€™t add up!
================================[1m Human Message [0m=================================

What did the cow say?
```
## é‡æ–°å®ç° ConversationTokenBufferMemory é€»è¾‘

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `trim_messages` æ¥ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œä¼šè¯ä¸­æœ€è¿‘çš„æ¶ˆæ¯ï¼Œå‰ææ˜¯ä¼šè¯ä¸­çš„æ€»ä»¤ç‰Œæ•°ä¸è¶…è¿‡æŸä¸ªé™åˆ¶ã€‚



```python
<!--IMPORTS:[{"imported": "trim_messages", "source": "langchain_core.messages", "docs": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}]-->
from langchain_core.messages import trim_messages

selected_messages = trim_messages(
    messages,
    # Please see API reference for trim_messages for other ways to specify a token counter.
    token_counter=ChatOpenAI(model="gpt-4o"),
    max_tokens=80,  # <-- token limit
    # The start_on is specified
    # to make sure we do not generate a sequence where
    # a ToolMessage that contains the result of a tool invocation
    # appears before the AIMessage that requested a tool invocation
    # as this will cause some chat models to raise an error.
    start_on=("human", "ai"),
    strategy="last",
    include_system=True,  # <-- Keep the system message
)

for msg in selected_messages:
    msg.pretty_print()
```
```output
================================[1m System Message [0m================================

you're a good assistant, you always respond with a joke.
================================[1m Human Message [0m=================================

why is 42 always the answer?
==================================[1m Ai Message [0m==================================

Because itâ€™s the only number thatâ€™s constantly right, even when it doesnâ€™t add up!
================================[1m Human Message [0m=================================

What did the cow say?
```
## ä½¿ç”¨ LangGraph çš„ç°ä»£ç”¨æ³•

ä¸‹é¢çš„ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ LangGraph æ·»åŠ ç®€å•çš„ä¼šè¯é¢„å¤„ç†é€»è¾‘ã€‚

:::note

å¦‚æœæ‚¨æƒ³é¿å…æ¯æ¬¡éƒ½åœ¨æ•´ä¸ªèŠå¤©å†å²ä¸Šè¿è¡Œè®¡ç®—ï¼Œå¯ä»¥éµå¾ª
å…³äºæ‘˜è¦çš„[æ“ä½œæŒ‡å—](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/)ï¼Œæ¼”ç¤ºäº†
å¦‚ä½•ä¸¢å¼ƒæ—§æ¶ˆæ¯ï¼Œç¡®ä¿å®ƒä»¬åœ¨åç»­å›åˆä¸­ä¸ä¼šè¢«é‡æ–°å¤„ç†ã€‚

:::

<details open>


```python
<!--IMPORTS:[{"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}]-->
import uuid

from IPython.display import Image, display
from langchain_core.messages import HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph

# Define a new graph
workflow = StateGraph(state_schema=MessagesState)

# Define a chat model
model = ChatOpenAI()


# Define the function that calls the model
def call_model(state: MessagesState):
    # highlight-start
    selected_messages = trim_messages(
        state["messages"],
        token_counter=len,  # <-- len will simply count the number of messages rather than tokens
        max_tokens=5,  # <-- allow up to 5 messages.
        strategy="last",
        # The start_on is specified
        # to make sure we do not generate a sequence where
        # a ToolMessage that contains the result of a tool invocation
        # appears before the AIMessage that requested a tool invocation
        # as this will cause some chat models to raise an error.
        start_on=("human", "ai"),
        include_system=True,  # <-- Keep the system message
        allow_partial=False,
    )

    # highlight-end
    response = model.invoke(selected_messages)
    # We return a list, because this will get added to the existing list
    return {"messages": response}


# Define the two nodes we will cycle between
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)


# Adding memory is straight forward in langgraph!
# highlight-next-line
memory = MemorySaver()

app = workflow.compile(
    # highlight-next-line
    checkpointer=memory
)


# The thread id is a unique key that identifies
# this particular conversation.
# We'll just generate a random uuid here.
thread_id = uuid.uuid4()
# highlight-next-line
config = {"configurable": {"thread_id": thread_id}}

input_message = HumanMessage(content="hi! I'm bob")
for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()

# Here, let's confirm that the AI remembers our name!
config = {"configurable": {"thread_id": thread_id}}
input_message = HumanMessage(content="what was my name?")
for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()
```
```output
================================[1m Human Message [0m=================================

hi! I'm bob
==================================[1m Ai Message [0m==================================

Hello Bob! How can I assist you today?
================================[1m Human Message [0m=================================

what was my name?
==================================[1m Ai Message [0m==================================

Your name is Bob. How can I help you, Bob?
```
</details>

## ä½¿ç”¨é¢„æ„å»ºçš„langgraphä»£ç†

æ­¤ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨[create_tool_calling_agent](https://api.python.langchain.com/en/latest/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html)å‡½æ•°æ„å»ºçš„é¢„æ„å»ºä»£ç†çš„ä»£ç†æ‰§è¡Œå™¨ã€‚

å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯[æ—§ç‰ˆLangChainé¢„æ„å»ºä»£ç†](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/)ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿ
ç”¨æ–°çš„[langgraphé¢„æ„å»ºä»£ç†](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/)æ›¿æ¢è¯¥ä»£ç ï¼Œè¯¥ä»£ç†åˆ©ç”¨
èŠå¤©æ¨¡å‹çš„åŸç”Ÿå·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œå¹¶ä¸”å¯èƒ½ä¼šæ›´å¥½åœ°å¼€ç®±å³ç”¨ã€‚

<details open>


```python
<!--IMPORTS:[{"imported": "AIMessage", "source": "langchain_core.messages", "docs": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "BaseMessage", "source": "langchain_core.messages", "docs": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "SystemMessage", "source": "langchain_core.messages", "docs": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "trim_messages", "source": "langchain_core.messages", "docs": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "tool", "source": "langchain_core.tools", "docs": "https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}]-->
import uuid

from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
    trim_messages,
)
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent


@tool
def get_user_age(name: str) -> str:
    """Use this tool to find the user's age."""
    # This is a placeholder for the actual implementation
    if "bob" in name.lower():
        return "42 years old"
    return "41 years old"


memory = MemorySaver()
model = ChatOpenAI()


# highlight-start
def state_modifier(state) -> list[BaseMessage]:
    """Given the agent state, return a list of messages for the chat model."""
    # We're using the message processor defined above.
    return trim_messages(
        state["messages"],
        token_counter=len,  # <-- len will simply count the number of messages rather than tokens
        max_tokens=5,  # <-- allow up to 5 messages.
        strategy="last",
        # The start_on is specified
        # to make sure we do not generate a sequence where
        # a ToolMessage that contains the result of a tool invocation
        # appears before the AIMessage that requested a tool invocation
        # as this will cause some chat models to raise an error.
        start_on=("human", "ai"),
        include_system=True,  # <-- Keep the system message
        allow_partial=False,
    )


# highlight-end

app = create_react_agent(
    model,
    tools=[get_user_age],
    checkpointer=memory,
    # highlight-next-line
    state_modifier=state_modifier,
)

# The thread id is a unique key that identifies
# this particular conversation.
# We'll just generate a random uuid here.
thread_id = uuid.uuid4()
config = {"configurable": {"thread_id": thread_id}}

# Tell the AI that our name is Bob, and ask it to use a tool to confirm
# that it's capable of working like an agent.
input_message = HumanMessage(content="hi! I'm bob. What is my age?")

for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()

# Confirm that the chat bot has access to previous conversation
# and can respond to the user saying that the user's name is Bob.
input_message = HumanMessage(content="do you remember my name?")

for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()
```
```output
================================[1m Human Message [0m=================================

hi! I'm bob. What is my age?
==================================[1m Ai Message [0m==================================
Tool Calls:
  get_user_age (call_jsMvoIFv970DhqqLCJDzPKsp)
 Call ID: call_jsMvoIFv970DhqqLCJDzPKsp
  Args:
    name: bob
=================================[1m Tool Message [0m=================================
Name: get_user_age

42 years old
==================================[1m Ai Message [0m==================================

Bob, you are 42 years old.
================================[1m Human Message [0m=================================

do you remember my name?
==================================[1m Ai Message [0m==================================

Yes, your name is Bob.
```
</details>

## LCELï¼šæ·»åŠ é¢„å¤„ç†æ­¥éª¤

æ·»åŠ å¤æ‚çš„å¯¹è¯ç®¡ç†çš„æœ€ç®€å•æ–¹æ³•æ˜¯åœ¨èŠå¤©æ¨¡å‹å‰å¼•å…¥ä¸€ä¸ªé¢„å¤„ç†æ­¥éª¤ï¼Œå¹¶å°†å®Œæ•´çš„å¯¹è¯å†å²ä¼ é€’ç»™é¢„å¤„ç†æ­¥éª¤ã€‚

è¿™ç§æ–¹æ³•åœ¨æ¦‚å¿µä¸Šå¾ˆç®€å•ï¼Œå¹¶ä¸”åœ¨è®¸å¤šæƒ…å†µä¸‹éƒ½èƒ½å·¥ä½œï¼›ä¾‹å¦‚ï¼Œå¦‚æœä½¿ç”¨[RunnableWithMessageHistory](/docs/how_to/message_history/)è€Œä¸æ˜¯åŒ…è£…èŠå¤©æ¨¡å‹ï¼Œåˆ™ç”¨é¢„å¤„ç†å™¨åŒ…è£…èŠå¤©æ¨¡å‹ã€‚

è¿™ç§æ–¹æ³•æ˜æ˜¾çš„ç¼ºç‚¹æ˜¯ï¼Œéšç€å¯¹è¯å†å²çš„å¢é•¿ï¼Œå»¶è¿Ÿå¼€å§‹å¢åŠ ï¼ŒåŸå› æœ‰ä¸¤ä¸ªï¼š

1. éšç€å¯¹è¯å˜é•¿ï¼Œå¯èƒ½éœ€è¦ä»æ‚¨ç”¨äºå­˜å‚¨å¯¹è¯å†å²çš„ä»»ä½•å­˜å‚¨ä¸­è·å–æ›´å¤šæ•°æ®ï¼ˆå¦‚æœä¸å°†å…¶å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼‰ã€‚
2. é¢„å¤„ç†é€»è¾‘æœ€ç»ˆä¼šè¿›è¡Œå¤§é‡å†—ä½™è®¡ç®—ï¼Œé‡å¤å¯¹è¯ä¹‹å‰æ­¥éª¤çš„è®¡ç®—ã€‚

:::caution

å¦‚æœæ‚¨æƒ³ä½¿ç”¨èŠå¤©æ¨¡å‹çš„å·¥å…·è°ƒç”¨åŠŸèƒ½ï¼Œè¯·è®°å¾—åœ¨å°†å†å²é¢„å¤„ç†æ­¥éª¤æ·»åŠ åˆ°æ¨¡å‹ä¹‹å‰ï¼Œå°†å·¥å…·ç»‘å®šåˆ°æ¨¡å‹ï¼

:::

<details open>


```python
<!--IMPORTS:[{"imported": "AIMessage", "source": "langchain_core.messages", "docs": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "BaseMessage", "source": "langchain_core.messages", "docs": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "HumanMessage", "source": "langchain_core.messages", "docs": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "SystemMessage", "source": "langchain_core.messages", "docs": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "trim_messages", "source": "langchain_core.messages", "docs": "https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "tool", "source": "langchain_core.tools", "docs": "https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}, {"imported": "ChatOpenAI", "source": "langchain_openai", "docs": "https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html", "title": "Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory"}]-->
from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
    trim_messages,
)
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

model = ChatOpenAI()


@tool
def what_did_the_cow_say() -> str:
    """Check to see what the cow said."""
    return "foo"


# highlight-start
message_processor = trim_messages(  # Returns a Runnable if no messages are provided
    token_counter=len,  # <-- len will simply count the number of messages rather than tokens
    max_tokens=5,  # <-- allow up to 5 messages.
    strategy="last",
    # The start_on is specified
    # to make sure we do not generate a sequence where
    # a ToolMessage that contains the result of a tool invocation
    # appears before the AIMessage that requested a tool invocation
    # as this will cause some chat models to raise an error.
    start_on=("human", "ai"),
    include_system=True,  # <-- Keep the system message
    allow_partial=False,
)
# highlight-end

# Note that we bind tools to the model first!
model_with_tools = model.bind_tools([what_did_the_cow_say])

# highlight-next-line
model_with_preprocessor = message_processor | model_with_tools

full_history = [
    SystemMessage("you're a good assistant, you always respond with a joke."),
    HumanMessage("i wonder why it's called langchain"),
    AIMessage(
        'Well, I guess they thought "WordRope" and "SentenceString" just didn\'t have the same ring to it!'
    ),
    HumanMessage("and who is harrison chasing anyways"),
    AIMessage(
        "Hmmm let me think.\n\nWhy, he's probably chasing after the last cup of coffee in the office!"
    ),
    HumanMessage("why is 42 always the answer?"),
    AIMessage(
        "Because itâ€™s the only number thatâ€™s constantly right, even when it doesnâ€™t add up!"
    ),
    HumanMessage("What did the cow say?"),
]


# We pass it explicity to the model_with_preprocesor for illustrative purposes.
# If you're using `RunnableWithMessageHistory` the history will be automatically
# read from the source the you configure.
model_with_preprocessor.invoke(full_history).pretty_print()
```
```output
==================================[1m Ai Message [0m==================================
Tool Calls:
  what_did_the_cow_say (call_urHTB5CShhcKz37QiVzNBlIS)
 Call ID: call_urHTB5CShhcKz37QiVzNBlIS
  Args:
```
</details>

å¦‚æœæ‚¨éœ€è¦å®ç°æ›´é«˜æ•ˆçš„é€»è¾‘ï¼Œå¹¶ä¸”æƒ³è¦ä½¿ç”¨ `RunnableWithMessageHistory`ï¼Œç›®å‰å®ç°æ­¤ç›®çš„çš„æ–¹æ³•
æ˜¯ä» [BaseChatMessageHistory](https://api.python.langchain.com/en/latest/chat_history/langchain_core.chat_history.BaseChatMessageHistory.html) å­ç±»åŒ–ï¼Œå¹¶
ä¸º `add_messages` å®šä¹‰é€‚å½“çš„é€»è¾‘ï¼ˆè€Œä¸æ˜¯ç®€å•åœ°é™„åŠ å†å²ï¼Œè€Œæ˜¯é‡å†™å®ƒï¼‰ã€‚

é™¤éæ‚¨æœ‰å……åˆ†çš„ç†ç”±å®ç°æ­¤è§£å†³æ–¹æ¡ˆï¼Œå¦åˆ™æ‚¨åº”è¯¥ä½¿ç”¨ LangGraphã€‚

## ä¸‹ä¸€æ­¥

æ¢ç´¢ä¸ LangGraph çš„æŒä¹…æ€§ï¼š

* [LangGraph å¿«é€Ÿå…¥é—¨æ•™ç¨‹](https://langchain-ai.github.io/langgraph/tutorials/introduction/)
* [å¦‚ä½•ä¸ºæ‚¨çš„å›¾æ·»åŠ æŒä¹…æ€§ï¼ˆâ€œè®°å¿†â€ï¼‰](https://langchain-ai.github.io/langgraph/how-tos/persistence/)
* [å¦‚ä½•ç®¡ç†å¯¹è¯å†å²](https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/)
* [å¦‚ä½•æ·»åŠ å¯¹è¯å†å²çš„æ‘˜è¦](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/)

ä½¿ç”¨ç®€å•çš„ LCEL æ·»åŠ æŒä¹…æ€§ï¼ˆå¯¹äºæ›´å¤æ‚çš„ç”¨ä¾‹è¯·ä½¿ç”¨ langgraphï¼‰ï¼š

* [å¦‚ä½•æ·»åŠ æ¶ˆæ¯å†å²](/docs/how_to/message_history/)

å¤„ç†æ¶ˆæ¯å†å²ï¼š

* [å¦‚ä½•ä¿®å‰ªæ¶ˆæ¯](/docs/how_to/trim_messages)
* [å¦‚ä½•è¿‡æ»¤æ¶ˆæ¯](/docs/how_to/filter_messages/)
* [å¦‚ä½•åˆå¹¶æ¶ˆæ¯è¿è¡Œ](/docs/how_to/merge_message_runs/)

