# GPT4All

本页面介绍如何在LangChain中使用`GPT4All`包装器。教程分为两个部分：安装和设置，随后是使用示例。

## 安装和设置

- 使用`pip install gpt4all`安装Python包
- 下载一个[GPT4All模型](https://gpt4all.io/index.html)并将其放置在您想要的目录中

在这个示例中，我们使用`mistral-7b-openorca.Q4_0.gguf`：

```bash
mkdir models
wget https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf -O models/mistral-7b-openorca.Q4_0.gguf
```

## 使用

### GPT4All

要使用 GPT4All 包装器，您需要提供预训练模型文件的路径和模型的配置。

```python
from langchain_community.llms import GPT4All

# Instantiate the model. Callbacks support token-wise streaming
model = GPT4All(model="./models/mistral-7b-openorca.Q4_0.gguf", n_threads=8)

# Generate text
response = model.invoke("Once upon a time, ")
```

您还可以自定义生成参数，例如 `n_predict`、`temp`、`top_p`、`top_k` 等。

要流式处理模型的预测，请添加 CallbackManager。

```python
from langchain_community.llms import GPT4All
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# There are many CallbackHandlers supported, such as
# from langchain.callbacks.streamlit import StreamlitCallbackHandler

callbacks = [StreamingStdOutCallbackHandler()]
model = GPT4All(model="./models/mistral-7b-openorca.Q4_0.gguf", n_threads=8)

# Generate text. Tokens are streamed through the callback manager.
model.invoke("Once upon a time, ", callbacks=callbacks)
```

## 模型文件

您可以从 GPT4All 客户端下载模型文件。您可以从 [GPT4All](https://gpt4all.io/index.html) 网站下载客户端。

有关此内容的更详细说明，请参见 [此笔记本](/docs/integrations/llms/gpt4all)
